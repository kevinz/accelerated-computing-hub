{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "05db90c8-a261-49ec-a4a2-c97cb8013ec7",
      "metadata": {},
      "source": [
       "# Numba\n",
       "\n",
       "[Numba](https://numba.pydata.org/) 是一个开源的即时编译器（JIT），可以将 Python 和 NumPy 代码的子集转换为快速的机器代码。\n",
       "\n",
       "Numba 通过直接将受限的 Python 代码子集编译为 CUDA 内核和设备函数来支持 CUDA GPU 编程，遵循 CUDA 执行模型。用 Numba 编写的内核看起来可以直接访问 NumPy 数组。NumPy 数组会在 CPU 和 GPU 之间自动传输。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1df11e1b-234c-4008-bf0c-bbdb897e0f07",
      "metadata": {},
      "source": [
       "## 什么是内核？\n",
       "\n",
       "内核类似于函数，它是一段接受输入并由处理器执行的代码块。\n",
       "\n",
       "函数和内核的区别在于：\n",
       "- 内核不能返回任何内容，它必须修改内存\n",
       "- 内核必须指定其线程层次结构（线程和块）"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "412bb30f-31d3-41b5-bec7-9d7dd72472fc",
      "metadata": {},
      "source": [
       "## 什么是网格、线程和块（以及线程束）？\n",
       "\n",
       "[线程和块](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)) 是您指示 GPU 并行处理代码的方式。我们的 GPU 是一个并行处理器，所以我们需要指定要执行内核的次数。\n",
       "\n",
       "线程之间有共享缓存内存的优势，但每个 GPU 上的核心数量有限，所以我们需要将工作分解成块，这些块将在 GPU 上被调度并并行运行。\n",
       "\n",
       "<figure>\n",
       "\n",
       "![CPU GPU 比较](images/threads-blocks-warps.png)\n",
       "\n",
       "<figcaption style=\"text-align: center;\"> \n",
       "    \n",
       "图片来源 <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a>\n",
       "    \n",
       "</figcaption>\n",
       "</figure>\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "fad6a21f-ce9c-49a2-9b6a-41fbd29bf6d1",
      "metadata": {},
      "source": [
       "### 什么？？\n",
       "\n",
       "现在不要太担心这个。只需记住**我们需要指定要调用内核的次数**，这是通过两个数字给出的，这两个数字相乘得到您的总网格大小。\n",
       "\n",
       "选择每个块的线程数的经验法则：\n",
       "- 应该是线程束大小（32）的倍数\n",
       "- 一个好的起点是每个块128-512个线程，但需要基准测试来确定最佳值。\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d36336db-8514-4b21-8072-62f085779f93",
      "metadata": {},
      "source": [
       "## Hello world"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e8dded05-6f39-4ea3-8ac6-45247ebc35d1",
      "metadata": {},
      "source": [
       "让我们深入研究一些代码，希望事情会变得更清楚。\n",
       "\n",
       "首先，让我们编写一个简单的基于 CPU 的 Python 函数，我们将在[列表推导式](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)中重复调用它。从 Python 的角度来看，列表推导式可以成为并行计算的一个很好的起点，因为它们已经感觉有点并行了。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a31223-019a-403b-bbd9-c3daa2f1042f",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "data = range(10)\n",
       "\n",
       "def foo(i):\n",
       "    return i\n",
       "    \n",
       "[foo(i) for i in data]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9c420044-2d0a-4ca6-8643-82f96390ff2a",
      "metadata": {},
      "source": [
       "这里我们的 `foo` 函数返回其索引值，并使用 `for` 循环遍历由 `range` 生成的数据。\n",
       "\n",
       "接下来，我们将把它转换为 CUDA 内核，并使用 Numba CUDA 在我们的 GPU 上运行它。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "81a1d262-f928-4473-b6b1-450bcb9fe60b",
      "metadata": {},
      "source": [
       "首先我们需要记住，我们的内核不能返回任何内容。相反，我们将使用一个输出列表来存储我们要返回的值。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8d3d9c-72fe-4ff7-b9f9-44e23dcf258e",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "data = range(10)\n",
       "output = []\n",
       "\n",
       "def foo(i):\n",
       "    output.append(i)\n",
       "    \n",
       "[foo(i) for i in data]\n",
       "\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9d161843-ee8d-4cb0-bf02-481c61b8b4e0",
      "metadata": {},
      "source": [
       "我们的下一个挑战是，我们在 GPU 上的输出数组必须有固定的长度。我们不能从一个空数组开始不断追加内容。所以让我们使用 NumPy 创建一个与输入数据长度相同的数组。我们还将把输入列表转换为 NumPy 数组，因为这是我们可以移动到 GPU 的内容。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "099b300d-0f4d-4f05-bbc7-b6b0e888f226",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "import numpy as np"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6832e81-5a23-4af2-bdd1-c562a21e6d33",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "data = np.asarray(range(10))\n",
       "output = np.zeros(len(data))\n",
       "\n",
       "def foo(i):\n",
       "    output[i] = i\n",
       "    \n",
       "[foo(i) for i in data]\n",
       "\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "15a42ff5-cc99-4889-acc5-443819664fdd",
      "metadata": {},
      "source": [
       "现在我们的纯 Python 函数表现得像一个内核，让我们使用 Numba 将它转换为内核。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60e9835-253f-4786-be3a-780f08df89da",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "from numba import cuda"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd640a5-ceaf-4fc1-9d87-9f9bda5d1d87",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "data = np.asarray(range(10))\n",
       "output = np.zeros(len(data))\n",
       "\n",
       "@cuda.jit\n",
       "def foo(input_array, output_array):\n",
       "    i = cuda.grid(1)\n",
       "    output_array[i] = input_array[i]\n",
       "    \n",
       "foo[1, len(data)](data, output)\n",
       "\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "758f7db8-9673-4b04-a7b3-2b8ef0e21907",
      "metadata": {},
      "source": [
       "**太棒了，上面的代码在我们的 GPU 上运行了！**\n",
       "\n",
       "现在让我们来分析一下这段代码。\n",
       "\n",
       "要将我们的 CPU 函数转换为 GPU 内核，我们需要添加 `@cuda.jit` 装饰器。这告诉 Numba 在运行时将我们的代码编译成与 CUDA 兼容的字节码。\n",
       "\n",
       "接下来，我们将内核的输入更改为 `input_array` 和 `output_array`。这是因为我们的内核需要引用这两个数组才能与它们交互。（稍后会详细介绍。）\n",
       "\n",
       "但是 `i` 呢？我们不是每次调用函数时都传递索引，而是可以依赖一个名为 `cuda.grid` 的好用的 CUDA 函数，它允许我们的内核在运行时获取自己的线程索引。\n",
       "\n",
       "最后，我们进行了一个看起来很奇怪的函数调用 `foo[blocks, threads](input, output)`。为了在 GPU 上并行运行我们的内核，我们需要指定要运行它的次数。内核函数使用方括号配置，并传递块大小和线程大小。由于我们的数组只有 `10` 个元素长，我们指定块大小为 `1`，线程大小为 `10`，这意味着我们的内核将执行 `10` 次。然后我们像往常一样传递参数。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f1d83f9b-c4c7-4b18-b83f-21c85cb0a5de",
      "metadata": {},
      "source": [
       "## 来点更大的\n",
       "\n",
       "现在我们已经用 Numba CUDA 运行了第一个 CUDA 内核，让我们尝试一些更大的东西。\n",
       "\n",
       "这次我们要取一个大数组并将其中的每个数字都乘以2。我们将首先在 CPU 上用纯 Python 做这件事，然后在 GPU 上用 CUDA 内核做这件事。\n",
       "\n",
       "让我们从一个包含3000万个随机数的大数组和一个相同长度的输出数组开始。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d06dc1c-b36e-48df-9c7f-0bf109c1ceae",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "random_array = np.random.random((30_000_000))\n",
       "random_array"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "2791c50f-a80b-4ccc-86a1-5955c508dc78",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "output = np.zeros_like(random_array)\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1d90ac1c-932a-4345-9e70-8a2e24f15c10",
      "metadata": {},
      "source": [
       "然后在 Python 中，让我们遍历这个数组并将每个项目乘以2放入输出数组。我们可以计时这个单元格来看看这需要多长时间。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "195b97db-2efc-4e64-8f95-3ade92c52a07",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "%%time\n",
       "\n",
       "def foo(i):\n",
       "    output[i] = random_array[i] * 2\n",
       "    \n",
       "[foo(i) for i in range(len(random_array))]\n",
       "\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d881d061-b924-48b5-97d9-8ce5bc862e69",
      "metadata": {},
      "source": [
       "对我来说，CPU 完成这个计算大约需要10秒。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "966920e3-653d-458c-9f9b-1e740e1db024",
      "metadata": {},
      "source": [
       "接下来，让我们编写一个完全相同的 CUDA 内核。与前面的示例唯一的区别是，我们将线程大小设置为固定值 `128`，然后计算需要多少个块才能覆盖整个数组。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98f0967-ad98-420f-9ccb-1eae9e242bc7",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "import math"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "9723dc8a-9627-442f-becc-3d9fc5423096",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "%%time\n",
       "\n",
       "output = np.zeros_like(random_array)\n",
       "\n",
       "threads = 128\n",
       "blocks = math.ceil(random_array.shape[0] / threads)\n",
       "\n",
       "@cuda.jit\n",
       "def foo(input_array, output_array):\n",
       "    i = cuda.grid(1)\n",
       "    output_array[i] = input_array[i] * 2\n",
       "    \n",
       "foo[blocks, threads](random_array, output)\n",
       "output"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "abec8daa-6d42-46cb-b95e-9f896de3a69d",
      "metadata": {},
      "source": [
       "太好了！这现在快了几个数量级，只需要几百毫秒。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f4043550-55eb-4a74-bdb7-32e97f58314e",
      "metadata": {},
      "source": [
       "精明的你可能会想，NumPy 已经是一个基于 C 的优化库，我们正在将我们的 GPU 内核与一些纯 Python 代码进行比较。如果我们用 NumPy 来做呢？\n",
       "\n",
       "你说得对，在这个例子中，NumPy 仍然比我们的 GPU 快。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1d6108-8d8f-4318-b7d8-f08cbf5c8b4d",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "%%time \n",
       "\n",
       "random_array * 2"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8b5a9d0e-775a-4871-8416-f81e232e728e",
      "metadata": {},
      "source": [
       "但这是因为内存管理。"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "383ba959-6e68-4f89-a887-8c139e847789",
      "metadata": {},
      "source": [
       "## 内存管理\n",
       "\n",
       "前面我们讨论过，CPU 和 GPU 实际上是两台独立的计算机。每台计算机都有自己的内存。\n",
       "\n",
       "到目前为止我们使用的所有数据都是用 CPU 上的 `numpy` 创建的。因此，为了让 `numba` 在 GPU 上处理这些数据，它一直在悄悄地为我们来回复制数据。\n",
       "\n",
       "这种数据移动会带来性能损失。\n",
       "\n",
       "我们也可以选择自己控制数据。我们可以使用 `cuda.to_device` 提前将数组显式移动到 GPU。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96de9eb-2adc-4179-89ce-5be1bdc2fb6b",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "gpu_random_array = cuda.to_device(random_array)\n",
       "gpu_output = cuda.to_device(np.zeros_like(random_array))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dccdba5-b280-448e-9d26-98e0120b283d",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "gpu_random_array"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0d552a86-08da-4324-bfc4-1c726aca528d",
      "metadata": {},
      "source": [
       "现在如果我们再次运行我们的内核并传递我们的 GPU 内存数组，我们看到它确实比 NumPy 表现更好。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "651b7b29-4bad-4015-af25-1040f15b1946",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "%%timeit -n 100\n",
       "foo[blocks, threads](gpu_random_array, gpu_output)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "39df78d6-6ea7-4252-9bcd-203d0920a81b",
      "metadata": {},
      "source": [
       "但是我们的输出结果仍然在 GPU 上。我们显式地将它复制到那里，所以我们需要使用 `copy_to_host()` 显式地将它复制回来。"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1d93c1-f54d-4c4f-a789-237ff74409d9",
      "metadata": {
       "tags": []
      },
      "outputs": [],
      "source": [
       "gpu_output.copy_to_host()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b6dc8ac8-433c-46e0-a978-065364089e18",
      "metadata": {},
      "source": [
       "这两个数据移动操作都需要时间。但是我们在这里执行的计算是微不足道的。随着内核中的计算变得更加复杂，复制数据所花费的时间百分比会变得越来越小。\n",
       "\n",
       "内存管理在其他地方也很有用。我们可能想要编写代码，在其中编写多个内核并将它们链接在一起。在每个内核调用之间将数据复制到 GPU 并返回将是低效的。\n",
       "\n",
       "```python\n",
       "# 将数组移动到 GPU\n",
       "foo[blocks, threads](data, output)\n",
       "# 将数据移回 CPU\n",
       "\n",
       "# 将数组移动到 GPU\n",
       "bar[blocks, threads](data, output)\n",
       "# 将数据移回 CPU\n",
       "\n",
       "# 将数组移动到 GPU\n",
       "baz[blocks, threads](data, output)\n",
       "# 将数据移回 CPU\n",
       "```\n",
       "\n",
       "因此，通过显式地将数据放在那里，我们可以减少这个时间并更好地控制我们的计算。\n",
       "\n",
       "```python\n",
       "# 将数组移动到 GPU\n",
       "data = cuda.to_device(data)\n",
       "output = cuda.to_device(output)\n",
       "\n",
       "foo[blocks, threads](data, output)\n",
       "bar[blocks, threads](data, output)\n",
       "baz[blocks, threads](data, output)\n",
       "\n",
       "# 将数据移回 CPU\n",
       "data = data.copy_to_host()\n",
       "output = output.copy_to_host()\n",
       "```"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }